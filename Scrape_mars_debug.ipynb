{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "from html.parser import HTMLParser\n",
    "import re\n",
    "\n",
    "\n",
    "def init_browser():\n",
    "    executable_path = {\"executable_path\": \"/usr/local/bin/chromedriver\"}\n",
    "    return Browser(\"chrome\", **executable_path, headless=False)\n",
    "\n",
    "\n",
    "def scrape_info():\n",
    "    browser = init_browser()\n",
    "\n",
    "    # scrape nasa url\n",
    "    url = \"https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest\"\n",
    "    browser.visit(url)\n",
    "\n",
    "    time.sleep(3)\n",
    "\n",
    "    # Scrape page into Soup\n",
    "    html = browser.html\n",
    "    soup = bs(html, \"html.parser\")\n",
    "\n",
    "    # Get article title\n",
    "    article = soup.find_all('div', class_='content_title')\n",
    "    # Get article body\n",
    "    body = soup.find_all('div', class_='article_teaser_body')\n",
    "\n",
    "    # Close the browser after scraping\n",
    "    browser.quit()\n",
    "    browser = init_browser()\n",
    "\n",
    "    # Get nasa images\n",
    "    url = \"https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars\"\n",
    "    browser.visit(url)\n",
    "\n",
    "    time.sleep(3)\n",
    "\n",
    "    # Scrape page into Soup\n",
    "    html = browser.html\n",
    "    soup = bs(html, \"html.parser\")\n",
    "\n",
    "    # scrape nasa images\n",
    "    image = soup.find('article', class_='carousel_item').get('style')\n",
    "\n",
    "    # Close the browser after scraping\n",
    "    browser.quit()\n",
    "    browser = init_browser()\n",
    "\n",
    "    # scrape nasa url\n",
    "    url = \"https://twitter.com/marswxreport?lang=en\"\n",
    "    browser.visit(url)\n",
    "\n",
    "    time.sleep(3)\n",
    "\n",
    "    # Scrape page into Soup\n",
    "    html = browser.html\n",
    "    soup = bs(html, \"html.parser\")\n",
    "\n",
    "    # get article body\n",
    "    tweet = soup.find_all('span', class_=\"css-901oao css-16my406 r-1qd0xha r-ad9z0x r-bcqeeo r-qvutc0\")\n",
    "    \n",
    "    # Close the browser after scraping\n",
    "    browser.quit()\n",
    "    browser = init_browser()\n",
    "\n",
    "    # scrape nasa url\n",
    "    url = \"https://space-facts.com/mars/\"\n",
    "    browser.visit(url)\n",
    "\n",
    "    time.sleep(3)\n",
    "\n",
    "    # Scrape page into Soup\n",
    "    html = browser.html\n",
    "    soup = bs(html, \"html.parser\")\n",
    "\n",
    "    # get article body\n",
    "    facts = soup.find_all('tr')\n",
    "    \n",
    "    # Close the browser after scraping\n",
    "    browser.quit()\n",
    "    browser = init_browser()\n",
    "\n",
    "    # scrape nasa url\n",
    "    url = \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\"\n",
    "    browser.visit(url)\n",
    "\n",
    "    time.sleep(3)\n",
    "\n",
    "    # Scrape page into Soup\n",
    "    html = browser.html\n",
    "    soup = bs(html, \"html.parser\")\n",
    "\n",
    "    # get article body\n",
    "    pics = soup.find_all('img', class_=\"thumb\")\n",
    "    \n",
    "    # Close the browser after scraping\n",
    "    browser.quit()\n",
    "\n",
    "    # function to remove html tags https://medium.com/@jorlugaqui/how-to-strip-html-tags-from-a-string-in-python-7cb81a2bbf44\n",
    "    def remove_html_tags(text):\n",
    "        clean = re.compile('<.*?>')\n",
    "        return re.sub(clean, '', text)\n",
    "    # call function and convert object to string using __repr__\n",
    "    check = remove_html_tags(article.__repr__())\n",
    "\n",
    "    # format data and split on any ,\n",
    "    save = check.split(\",\")\n",
    "    title_clean = save\n",
    "\n",
    "    # # create and clean new dataframe\n",
    "    title_clean = pd.DataFrame(title_clean)\n",
    "    title_clean.columns = ['title']\n",
    "    title_df = title_clean.drop([0, 13, 15, 32, 38, 39, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58])\n",
    "    title_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # call function and convert object to string using __repr__\n",
    "    check_bodies = remove_html_tags(body.__repr__())\n",
    "\n",
    "    # format data and split on any ,\n",
    "    save_body = check_bodies.replace(\"[\", '').split(\"., \")\n",
    "\n",
    "    # create and clean new dataframe\n",
    "    body_df = pd.DataFrame(save_body)\n",
    "    body_df.columns = ['Body']\n",
    "    body_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # put body and title into 1 df\n",
    "    new_df = pd.concat([title_df, body_df], axis=1)\n",
    "    \n",
    "    #remove characters from image scrape\n",
    "    image = image.replace(\"background-image: url(\", \"\")\n",
    "    image = image.replace(\");\", \"\")\n",
    "    image = image.replace(\"\"\"'\"\"\", \"\")\n",
    "\n",
    "    # call function and convert object to string using __repr__\n",
    "    tweet = remove_html_tags(tweet.__repr__())\n",
    "    tweet = tweet.split(\"Â·,\")\n",
    "    \n",
    "    #create df and reformat\n",
    "    tweet = pd.DataFrame(tweet)\n",
    "    tweet.columns = ['tweet']\n",
    "    tweet.drop(tweet.index[[0,1,2,4,5,6,7,8]], inplace=True)\n",
    "    tweet.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    #save what we are targeting\n",
    "    news_title = title_df.iloc[0]['title']\n",
    "    news_p = body_df.iloc[0]['Body']\n",
    "    featured_image_url = f\"https://www.jpl.nasa.gov{image}\"\n",
    "    mars_weather = tweet\n",
    "\n",
    "    # call function and convert object to string using __repr__\n",
    "    facts = remove_html_tags(facts.__repr__())\n",
    "    \n",
    "    # edit facts df\n",
    "    facts = facts.replace(\"[\", \"\").split(\", \")\n",
    "    facts_df = pd.DataFrame(facts)\n",
    "    facts_df.columns = ['facts']\n",
    "    facts_df = facts_df['facts'].str.split(\":\", expand = True)\n",
    "    facts_df.columns = ['Info', 'Data']\n",
    "    \n",
    "    #last of the scrapes\n",
    "    news_title = title_df.iloc[0]['title']\n",
    "    news_p = body_df.iloc[0]['Body']\n",
    "    featured_image_url = f\"https://www.jpl.nasa.gov{image}\"\n",
    "    mars_weather = tweet\n",
    "    facts_df\n",
    "    mars_list = {\n",
    "        \"title\" : \"Cerberus Hemisphere\", \"img_url\" : \"/cache/images/dfaf3849e74bf973b59eb50dab52b583_cerberus_enhanced.tif_thumb.png\",\n",
    "        \"title\" : \"Schiaparelli Hemisphere\", \"img_url\" : \"cache/images/7677c0a006b83871b5a2f66985ab5857_schiaparelli_enhanced.tif_thumb.png\",\n",
    "        \"title\" : \"Syrtis Major\", \"img_url\" : \"/cache/images/aae41197e40d6d4f3ea557f8cfe51d15_syrtis_major_enhanced.tif_thumb.png\",\n",
    "        \"title\" : \"Valles Marineris\", \"img_url\" : \"/cache/images/04085d99ec3713883a9a57f42be9c725_valles_marineris_enhanced.tif_thumb.png\"\n",
    "    }\n",
    "    #create list to store all scrapes\n",
    "    mars_data = {\n",
    "        \"news_title\" : news_title,\n",
    "        \"news_para\" : news_p,\n",
    "        \"featured_image_url\" : featured_image_url,\n",
    "        \"mars_weather\" : mars_weather.encode('utf-8', 'strict'),\n",
    "        \"facts_table\" : facts_df.encode('utf-8', 'strict'),\n",
    "        \"mars_list\" : {\"title\" : \"Cerberus Hemisphere\", \"img_url\" : \"/cache/images/dfaf3849e74bf973b59eb50dab52b583_cerberus_enhanced.tif_thumb.png\",\n",
    "                \"title\" : \"Schiaparelli Hemisphere\", \"img_url\" : \"cache/images/7677c0a006b83871b5a2f66985ab5857_schiaparelli_enhanced.tif_thumb.png\",\n",
    "                \"title\" : \"Syrtis Major\", \"img_url\" : \"/cache/images/aae41197e40d6d4f3ea557f8cfe51d15_syrtis_major_enhanced.tif_thumb.png\",\n",
    "                \"title\" : \"Valles Marineris\", \"img_url\" : \"/cache/images/04085d99ec3713883a9a57f42be9c725_valles_marineris_enhanced.tif_thumb.png\"},\n",
    "    }\n",
    "    return mars_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c6accd585bc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscrape_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-046b4b647c06>\u001b[0m in \u001b[0;36mscrape_info\u001b[0;34m()\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"news_para\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mnews_p\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;34m\"featured_image_url\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mfeatured_image_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0;34m\"mars_weather\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mmars_weather\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'strict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m         \u001b[0;34m\"facts_table\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mfacts_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'strict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \"mars_list\" : {\"title\" : \"Cerberus Hemisphere\", \"img_url\" : \"/cache/images/dfaf3849e74bf973b59eb50dab52b583_cerberus_enhanced.tif_thumb.png\",\n",
      "\u001b[0;32m/opt/anaconda3/envs/PythonData/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5177\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5178\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "scrape_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
